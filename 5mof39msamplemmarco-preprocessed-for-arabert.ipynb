{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## checkout this papers:\n\n[mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset](https://arxiv.org/abs/2108.13897)\n\n[A cost-benefit analysis of cross-lingual transfer methods](https://arxiv.org/abs/2105.06813)\n","metadata":{}},{"cell_type":"code","source":"#load the mMARCO a multilingual version of the MS MARCO passage ranking dataset \n#from huggingface https://huggingface.co/datasets/unicamp-dl/mmarco\nfrom datasets import load_dataset\ndataset = load_dataset('unicamp-dl/mmarco', 'arabic')\ndataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://huggingface.co/aubmindlab/araelectra-base-discriminator\n# A preprocessing is recommended by the authors of AraELECTRA and AraBERT before training or testing on any dataset. \n!pip install arabert -q\nfrom arabert.preprocess import ArabertPreprocessor\n\n#model_name=\"araelectra-base\"\nmodel_name=\"bert-base-arabertv2\"\narabert_prep = ArabertPreprocessor(model_name=model_name)\n\n#text = \"و لن نبالغ إذا قلنا إن الهاتف أ و كمبيوتر  المكتب في زمننا هذا ضروري\"\n#arabert_prep.preprocess(text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select 5M sample out of 39M sample\ndataset_eval = dataset['train'].select(range(5000000, 5005000))\ndataset_train = dataset['train'].select(range(0, 5000000))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The dataset is in the form (query, positive passage, negative passage).\n# We split it into the forms (query, positive passage, label=1) and (query, negative passage, label=0)\n# and preprocessed it, preprocessing with bert-base-arabertv2 take more than 12 hours it exceed the limit of Kaggle Notbook\n#You can run this code for a range of 2.5M samples, then concatenate the resulted dataset. \n#from datasets import load_dataset, load_from_disk, concatenate_datasets\n#dataset0 = load_from_disk('path to dataset0')\n#dataset1 = load_from_disk('path to dataset1')\n#dataset = concatenate_datasets([dataset0, dataset1])\ndef split_examples(batch):\n    queries = []\n    passages = []\n    labels = []\n    for label in [\"positive\", \"negative\"]:\n        for (query, passage) in zip(batch[\"query\"], batch[label]):\n            queries.append(arabert_prep.preprocess(query))\n            passages.append(arabert_prep.preprocess(passage))\n            labels.append(int(label == \"positive\"))\n    return {\"query\": queries, \"passage\": passages, \"label\": labels}\n\ndataset_train = dataset_train.map(split_examples, batched=True, remove_columns=[\"positive\", \"negative\"])\ndataset_eval = dataset_eval.map(split_examples, batched=True, remove_columns=[\"positive\", \"negative\"])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we apply tokenization \nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nargs_model=\"aubmindlab/araelectra-base-discriminator\"\ntokenizer = AutoTokenizer.from_pretrained(args_model)\n\ndef tokenize(batch):\n    tokenized = tokenizer(\n        batch[\"query\"],\n        batch[\"passage\"],\n        padding=True,\n        truncation=\"only_second\",\n        max_length=512,\n        )\n    tokenized[\"labels\"] = [[float(label)] for label in batch[\"label\"]]\n    return tokenized","metadata":{"execution":{"iopub.status.busy":"2024-05-17T14:53:33.733142Z","iopub.execute_input":"2024-05-17T14:53:33.733738Z","iopub.status.idle":"2024-05-17T14:53:45.877634Z","shell.execute_reply.started":"2024-05-17T14:53:33.733705Z","shell.execute_reply":"2024-05-17T14:53:45.876225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = dataset_train.map(tokenize, batched=True, remove_columns=[\"query\", \"passage\", \"label\"])\ndataset_train.set_format(\"torch\")\ndataset_eval = dataset_eval.map(tokenize, batched=True, remove_columns=[\"query\", \"passage\", \"label\"])\ndataset_eval.set_format(\"torch\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the dataset locally\ndataset_train.save_to_disk(\"mmarco_train10M_preprossesd_for_AraBERT\")\ndataset_eval.save_to_disk(\"mmarco_eval10k_preprossesd_for_AraBERT\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub \nhf = huggingface_hub.HfFolder()\naccess_token = \"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\" \norganization_dataset_id=\"hatemestinbejaia/RARAELECTRAandRARABERTusedDATASET\"\n#To push the dataset to your own Huggingface repository, change the organization_dataset_id and access_token\nhf.save_token(access_token)\ndataset_train.push_to_hub(organization_dataset_id, \"mmarco_train10M_preprossesd_for_AraBERT\")\ndataset_eval.push_to_hub(organization_dataset_id, \"mmarco_eval10k_preprossesd_for_AraBERT\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#You can use the processed dataset directly from our repository to fine-tune your owen version-based AraELECTRA\n#using the below code \nfrom datasets import load_dataset\ndataset_train = load_dataset(organization_dataset_id, 'mmarco_train10M_preprossesd_for_AraBERT')\ndataset_eval = load_dataset(organization_dataset_id, 'mmarco_eval10k_preprossesd_for_AraBERT')","metadata":{},"execution_count":null,"outputs":[]}]}